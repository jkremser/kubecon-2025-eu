apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: "1"
    serving.kserve.io/deploymentMode: RawDeployment
  labels:
    app: llama
  name: llama
spec:
  selector:
    matchLabels:
      app: llama
  strategy:
    type: Recreate
  template:
    metadata:
      annotations:
        serving.kserve.io/deploymentMode: RawDeployment
        sidecar.opentelemetry.io/inject: "true"
      labels:
        app: llama
    spec:
      containers:
      - args:
        - --model=/mnt/models/llama3/
        - --port=8080
        - --served-model-name=llama3
        - --load-format=safetensors
        - --kv-cache-dtype=auto
        - --guided-decoding-backend=outlines
        - --tensor-parallel-size=1
        - --gpu-memory-utilization=0.99
        - --max-num-batched-tokens=2048
        - --max-model-len=2048
        - --enable-auto-tool-choice
        - --tool-call-parser=llama3_json
        - --dtype=float16
        image: docker.io/vllm/vllm-openai:v0.6.4
        imagePullPolicy: IfNotPresent
        name: main
        volumeMounts:
        - mountPath: /mnt/models/
          name: model
          readOnly: true
        ports:
        - containerPort: 8080
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: 8080
          timeoutSeconds: 1
        resources:
          limits:
            cpu: "4"
            memory: 16Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "4"
            memory: 8Gi
            nvidia.com/gpu: "1"
      volumes:
      - name: model
        persistentVolumeClaim:
          claimName: models-pvc-clone
          readOnly: true
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - llama
              topologyKey: kubernetes.io/hostname
          # requiredDuringSchedulingIgnoredDuringExecution:
          # - labelSelector:
          #     matchExpressions:
          #     - key: app
          #       operator: In
          #       values:
          #       - llama
          #   topologyKey: "kubernetes.io/hostname"
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: llama
  name: llama
spec:
  ports:
  - port: 8080
  selector:
    app: llama
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama
spec:
  ingressClassName: nginx
  rules:
  - host: model
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llama
            port:
              number: 8080
